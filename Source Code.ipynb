{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f842ab45",
   "metadata": {},
   "source": [
    "# Keyword Extraction Using LDA Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "089c4b02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall topics for category \u001b[1m 'food'\u001b[0m:\n",
      "Topic 1: recipe\n",
      "Topic 2: foodie\n",
      "Topic 3: chicken\n",
      "Topic 4: food\n",
      "Topic 5: homemade\n",
      "Topic 6: place\n",
      "Topic 7: biryani\n",
      "Topic 8: platter\n",
      "\n",
      "Overall topics for category \u001b[1m 'GiftShop'\u001b[0m:\n",
      "Topic 1: box\n",
      "Topic 2: loved\n",
      "Topic 3: gift\n",
      "Topic 4: pakistan\n",
      "Topic 5: lahore\n",
      "Topic 6: mahionlinegift\n",
      "Topic 7: giftboutique\n",
      "Topic 8: day\n",
      "\n",
      "Overall topics for category \u001b[1m 'Clothing'\u001b[0m:\n",
      "Topic 1: embroidery\n",
      "Topic 2: dress\n",
      "Topic 3: summer\n",
      "Topic 4: eid\n",
      "Topic 5: radiates\n",
      "Topic 6: perfect\n",
      "Topic 7: ochre\n",
      "Topic 8: online\n",
      "\n",
      "Overall topics for category \u001b[1m 'Beauty'\u001b[0m:\n",
      "Topic 1: makeup\n",
      "Topic 2: makeupartist\n",
      "Topic 3: beauty\n",
      "Topic 4: concealer\n",
      "Topic 5: skincare\n",
      "Topic 6: ofraglow\n",
      "Topic 7: highlighter\n",
      "Topic 8: crueltyfreemakeup\n",
      "\n",
      "Overall topics for category \u001b[1m 'Fitness'\u001b[0m:\n",
      "Topic 1: fitness\n",
      "Topic 2: exercise\n",
      "Topic 3: fat\n",
      "Topic 4: need\n",
      "Topic 5: weightlossjourney\n",
      "Topic 6: bodypositivity\n",
      "Topic 7: lowcarbvegan\n",
      "Topic 8: difference\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#*****************************************************************#\n",
    "#Cleaning the Data ----Preprocessing\n",
    "#*****************************************************************#\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#LDA Module which can Resolve the Error \n",
    "#import gensim \n",
    "#from gensim import corpora\n",
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "# Load the Excel file into a pandas dataframe\n",
    "df = pd.read_excel('dataset.xlsx')\n",
    "#print(df)\n",
    "# Remove irrelevant columns\n",
    "df = df[['category', 'username', 'captions', 'hashtags']]\n",
    "\n",
    "# Handle missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Remove punctuation and special characters\n",
    "df['captions'] = df['captions'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "df['hashtags'] = df['hashtags'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "# Define the pattern to match numeric values\n",
    "pattern = r'\\d+'\n",
    "\n",
    "# Remove numeric values from captions column\n",
    "df['captions'] = df['captions'].apply(lambda x: re.sub(pattern, '', x))\n",
    "\n",
    "# Remove numeric values from hashtags column\n",
    "df['hashtags'] = df['hashtags'].apply(lambda x: re.sub(pattern, '', x))\n",
    "\n",
    "# Convert text to lowercase\n",
    "df['captions'] = df['captions'].apply(lambda x: x.lower())\n",
    "df['hashtags'] = df['hashtags'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['captions'] = df['captions'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "df['hashtags'] = df['hashtags'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "#print(df)\n",
    "# Lemmatize the text\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['captions'] = df['captions'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "df['hashtags'] = df['hashtags'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "# Create a corpus for each page separately\n",
    "corpora = []\n",
    "pages = df['username'].unique()\n",
    "for page in pages:\n",
    "    page_df = df[df['username'] == page]\n",
    "    page_corpus = page_df['captions'] + ' ' + page_df['hashtags']\n",
    "    corpora.append(page_corpus)\n",
    "\n",
    "#WordtoVec vector representation\n",
    "#Clustering technique \n",
    "\n",
    "#*****************************************************************#\n",
    "#Performing Topic Modelling ---LDA to get the keywords per page \n",
    "#*****************************************************************#\n",
    "    \n",
    "# Apply LDA per page\n",
    "num_topics =  8 # Specify the number of topics to extract for each page\n",
    "keyword_results = []\n",
    "for corpus in corpora:\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_topics)\n",
    "    lda_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "    #print(lda_matrix)\n",
    "    top_keywords = []\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        #print(lda_model.components_)\n",
    "        #print(\"--------------------Topicidx\")\n",
    "        #print(topic_idx)\n",
    "        topic_keywords = [feature_names[i] for i in topic.argsort()[:-6:-1]]  # Get top 5 keywords\n",
    "        top_keywords.append(topic_keywords)\n",
    "        #top_keywords.append((topic_keywords, lda_model.transform(tfidf_matrix)))\n",
    "\n",
    "    keyword_results.append(top_keywords)\n",
    "\n",
    "\n",
    "#*****************************************************************#\n",
    "#Performing Topic Modelling ---LDA to get the keywords per Category \n",
    "#*****************************************************************#\n",
    "    \n",
    "category_keywords = {}\n",
    "for i, page in enumerate(pages):\n",
    "    category = df.loc[df['username'] == page, 'category'].iloc[0]\n",
    "    if category not in category_keywords:\n",
    "        category_keywords[category] = []\n",
    "    category_keywords[category].extend(keyword_results[i])\n",
    "\n",
    "# Flatten the keywords list\n",
    "category_keywords_flat = {category: [keyword for sublist in keywords for keyword in sublist]\n",
    "                          for category, keywords in category_keywords.items()}\n",
    "\n",
    "# Perform second iteration of topic modeling for each category\n",
    "category_topics = {}\n",
    "for category, keywords in category_keywords_flat.items():\n",
    "    keyword_counts = Counter(keywords)\n",
    "    top_keywords = keyword_counts.most_common(num_topics)\n",
    "    category_topics[category] = [keyword for keyword, count in top_keywords]\n",
    "\n",
    "# Print the overall topics for each category\n",
    "for category, topics in category_topics.items():\n",
    "    print(f\"\\nOverall topics for category \\033[1m '{category}'\\033[0m:\")\n",
    "    for i, topic in enumerate(topics):\n",
    "        #print(f\"Topic {i+1}: {', '.join(topic)}\")\n",
    "        print(f\"Topic {i+1}:\",topic)\n",
    "        #print(topic)\n",
    "    #print()\n",
    "\n",
    "\n",
    "# Calculate the score of each keyword\n",
    "category_keyword_scores = {}\n",
    "keyword_frequency_dict={}\n",
    "for category, keywords in category_topics.items():\n",
    "    keyword_scores = {}\n",
    "    sum_keyword_scores = 0\n",
    "    for keyword in keywords:\n",
    "        keyword_score = 0\n",
    "        for i, page_corpus in enumerate(corpora):\n",
    "            # Calculate the frequency of the keyword in the page corpus\n",
    "            keyword_frequency = page_corpus.str.count(keyword).sum()\n",
    "            #print(pages)\n",
    "            #print(keyword_frequency)\n",
    "\n",
    "             # Add the keyword frequency to the keyword frequency dictionary\n",
    "            page_name = pages[i]\n",
    "            if page_name not in keyword_frequency_dict:\n",
    "                keyword_frequency_dict[page_name] = {}\n",
    "            keyword_frequency_dict[page_name][keyword] = keyword_frequency\n",
    "            #print(keyword_frequency_dict[page_name])\n",
    "            # Calculate the total number of words in the page corpus\n",
    "            total_words = len(' '.join(page_corpus.tolist()).split())\n",
    "            # Calculate the frequency of the keyword normalized by the total number of words\n",
    "            #normalized_frequency = keyword_frequency / total_words\n",
    "            #print(lda_matrix.shape[0])\n",
    "            # Add the normalized frequency of the keyword to the keyword score for the page\n",
    "            #keyword_score += normalized_frequency * lda_matrix[i][np.argmax(lda_matrix[i])]\n",
    "        # Calculate the average keyword score across all pages\n",
    "        #keyword_score /= len(corpora)\n",
    "        #keyword_scores[keyword] = keyword_score\n",
    "        #print()\n",
    "        #sum_keyword_scores += keyword_score\n",
    "    # Normalize the score of each keyword\n",
    "    #for keyword in keyword_scores:\n",
    "        #keyword_scores[keyword] /= sum_keyword_scores\n",
    "    #category_keyword_scores[category] = keyword_scores\n",
    "\n",
    "# Print the scores of the top keywords for each category\n",
    "#for category, keyword_scores in category_keyword_scores.items():\n",
    "    #print(f\"Scores of the top keywords for category '{category}':\")\n",
    "    #for i, (keyword, score) in enumerate(sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)[:num_topics]):\n",
    "        #print(f\"Keyword {i+1}: '{keyword}', Score: {score:.3f}\")\n",
    "    #print()\n",
    "    \n",
    "    \n",
    "#print(\"---------------------------\")\n",
    "#print(category_topics.items())    \n",
    "#print(\"---------------------------\")\n",
    "#print(\"---------------------------\")\n",
    "\n",
    "#for i in pages:\n",
    "    #print(keyword_frequency_dict[i])\n",
    "    \n",
    "#print(\"---------------------------Category Topics-------------------------\")\n",
    "   \n",
    "#print(category_topics)    \n",
    "\n",
    "#print(\"---------------------------Pages/keywords/Frequency-------------------------\")\n",
    "\n",
    "#print(keyword_frequency_dict.items())    \n",
    "\n",
    "\n",
    "#*****************************************************************#\n",
    "#Creating Excel FIle for passing the data to our Learning Model\n",
    "#*****************************************************************#\n",
    "\n",
    "# create an empty list to store the data\n",
    "data_list = []\n",
    "\n",
    "# loop through the pages and keywords to extract the data\n",
    "for page_name, keyword_frequency_dict in keyword_frequency_dict.items():\n",
    "    # find the category for the current page\n",
    "    category = None\n",
    "    for category_name, pages in category_topics.items():\n",
    "        #print(\"***********************\")\n",
    "        #print(category_name)\n",
    "        #print(pages)\n",
    "        #if page_name in pages:\n",
    "            category = df.loc[df['username'] == page_name, 'category'].iloc[0]   \n",
    "            #break\n",
    "    \n",
    "    # extract the keywords and frequencies for the page\n",
    "    keywords = [keyword for keyword, _ in keyword_frequency_dict.items()]\n",
    "    frequencies = [frequency for _, frequency in keyword_frequency_dict.items()]\n",
    "    \n",
    "    # append the data to the list\n",
    "    data_list.append((page_name, *frequencies,category))\n",
    "\n",
    "# create a dataframe from the list\n",
    "df = pd.DataFrame(data_list, columns=[\"username\", *keywords,\"category\"])\n",
    "\n",
    "# write the dataframe to an excel file\n",
    "df.to_excel(\"trainingset.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc756a21",
   "metadata": {},
   "source": [
    "# Training of Model on Training Set Using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f6f169c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CategoryPrediction.joblib']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "import joblib\n",
    "\n",
    "# Load the data from the output.xlsx file\n",
    "data = pd.read_excel('trainingset.xlsx', index_col=0)\n",
    "\n",
    "# Split the data into input features and labels\n",
    "X = data.iloc[:, :-1]\n",
    "#print(X)\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Create an SVM classifier object\n",
    "clf = svm.SVC(kernel='poly')\n",
    "\n",
    "# Train the SVM model on the data\n",
    "clf.fit(X, y)\n",
    "\n",
    "\n",
    "# Save the trained model in disk\n",
    "joblib.dump(clf, 'CategoryPrediction.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14933775",
   "metadata": {},
   "source": [
    "# Cleaning of Testing File and Creating Testing File for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "01b85c7e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Excel File Created for Testing the Model to pass in the Model to Test\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from collections import Counter\n",
    "\n",
    "# Load the Excel file into a pandas dataframe\n",
    "df = pd.read_excel('testingset.xlsx')\n",
    "#print(df)\n",
    "# Remove irrelevant columns\n",
    "df = df[['username', 'captions', 'hashtags']]\n",
    "\n",
    "# Handle missing values\n",
    "#df.dropna(inplace=True)\n",
    "\n",
    "# Remove punctuation and special characters\n",
    "df['captions'] = df['captions'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "df['hashtags'] = df['hashtags'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "# Convert text to lowercase\n",
    "df['captions'] = df['captions'].apply(lambda x: x.lower())\n",
    "df['hashtags'] = df['hashtags'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['captions'] = df['captions'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "df['hashtags'] = df['hashtags'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# Lemmatize the text\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['captions'] = df['captions'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "df['hashtags'] = df['hashtags'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "# Create a corpus for each page separately\n",
    "corpora = []\n",
    "pages = df['username'].unique()\n",
    "#print(pages)\n",
    "for page in pages:\n",
    "    page_df = df[df['username'] == page]\n",
    "    page_corpus = page_df['captions'] + ' ' + page_df['hashtags']\n",
    "    corpora.append(page_corpus)\n",
    "#print(corpora)\n",
    "\n",
    "\n",
    "\n",
    "#**********************************************#\n",
    "# Get the list of column names from the subset\n",
    "#**********************************************#\n",
    "df = pd.read_excel('trainingset.xlsx')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "subset_df = df.iloc[:,1 :-1]\n",
    "#print(subset_df)\n",
    "column_names = subset_df.columns.tolist()\n",
    "\n",
    "#print(column_names)\n",
    "column_dict = {\"column_names\": column_names}\n",
    "#print(column_dict)\n",
    "# Print the column names for each username\n",
    "#for username, columns in column_dict.items():\n",
    "    #print(f\"Columns for username '{username}': {columns}\")\n",
    "#**********************************************#\n",
    "#Frequency of the Words Features present in Testing FIle #\n",
    "#**********************************************#\n",
    "\n",
    "#print(column_dict.items())\n",
    "\n",
    "column_frequency_dict={}\n",
    "\n",
    "for user,keywords in column_dict.items():\n",
    "    keyword_scores = {}\n",
    "    sum_keyword_scores = 0\n",
    "    for keyword in keywords:\n",
    "        keyword_score = 0\n",
    "        for i, page_corpus in enumerate(corpora):\n",
    "            # Calculate the frequency of the keyword in the page corpus\n",
    "            keyword_frequency = page_corpus.str.count(keyword).sum()\n",
    "            #print(pages)\n",
    "            #print(keyword_frequency)\n",
    "\n",
    "             # Add the keyword frequency to the keyword frequency dictionary\n",
    "            page_name = pages[i]\n",
    "            if page_name not in column_frequency_dict:\n",
    "                column_frequency_dict[page_name] = {}\n",
    "            column_frequency_dict[page_name][keyword] = keyword_frequency\n",
    "            #print(column_frequency_dict[page_name])\n",
    "\n",
    "\n",
    "#**********************************************#\n",
    "#Forming Testing File to Test the Model #\n",
    "#**********************************************#\n",
    "           \n",
    "# Get the list of unique usernames\n",
    "usernames = df['username'].unique()\n",
    "    \n",
    "\n",
    "# Convert the frequency dictionary into a DataFrame\n",
    "frequency_df = pd.DataFrame.from_dict(column_frequency_dict)\n",
    "\n",
    "# Transpose the DataFrame so that the usernames are in the rows and the keywords are in the columns\n",
    "frequency_df = frequency_df.transpose()\n",
    "\n",
    "# Add a column for the usernames\n",
    "frequency_df.insert(0, 'username', frequency_df.index)\n",
    "\n",
    "# Reset the index\n",
    "frequency_df = frequency_df.reset_index(drop=True)\n",
    "\n",
    "# Write the DataFrame to an Excel file\n",
    "frequency_df.to_excel('modeltesting.xlsx', index=False)\n",
    "print(\"\\033[1m Excel File Created for Testing the Model to pass in the Model to Test\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5539c65",
   "metadata": {},
   "source": [
    "# Testing the Model trained on SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "699ae7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Model Trained on SVM using kernel = rbf \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "# Load the model\n",
    "model = joblib.load('CategoryPrediction.joblib')\n",
    "\n",
    "# Load the testing data\n",
    "testing_df = pd.read_excel(\"modeltesting.xlsx\")\n",
    "\n",
    "# Use all columns except the first as input features\n",
    "#features_df = testing_df.iloc[:, 0:]\n",
    "features_df = testing_df.iloc[:, 1:]\n",
    "#features_df = testing_df.iloc[:, 2:]\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "#print(features_df)\n",
    "# Predict the category for each row using the model\n",
    "predictions = model.predict(features_df)\n",
    "\n",
    "# Add the predicted category to the original testing dataframe\n",
    "testing_df[\"predicted_category\"] = predictions\n",
    "\n",
    "# Save the results to a new excel file\n",
    "testing_df.to_excel(\"SVM_modeltesting_results.xlsx\", index=False)\n",
    "print(\"\\033[1m Model Trained on SVM using kernel = rbf \\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a085cd2",
   "metadata": {},
   "source": [
    "# Appending the Actual Category of the Pages of Testing File in the SVM  Model Results File to get Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "83abfe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the original DataFrame from the file\n",
    "df = pd.read_excel('testingset.xlsx')\n",
    "\n",
    "# Get the last column (actual category) and the username column\n",
    "last_column = df.iloc[:, -1]\n",
    "username_column = df['username']\n",
    "\n",
    "# Create a dictionary to store unique username and corresponding actual category\n",
    "username_category_dict = {}\n",
    "\n",
    "# Iterate over the username and last column\n",
    "for username, category in zip(username_column, last_column):\n",
    "    if username not in username_category_dict:\n",
    "        # If the username is not already in the dictionary, add it with the corresponding category\n",
    "        username_category_dict[username] = category\n",
    "\n",
    "# Read the SVMresults.xlsx file\n",
    "result_df = pd.read_excel('SVM_modeltesting_results.xlsx')\n",
    "\n",
    "# Create a new column to store the actual category based on unique username\n",
    "result_df['actualcategory'] = result_df['username'].map(username_category_dict)\n",
    "\n",
    "# Write the updated DataFrame back to the SVMresults.xlsx file\n",
    "result_df.to_excel('SVM_modeltesting_results.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6234ba10",
   "metadata": {},
   "source": [
    "# Calculating Performance Metrics of Model SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8a98f11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m**********************SVM Model Performance***************************\n",
      "\u001b[0m\n",
      "\u001b[1mOverall Accuracy of SVM Model using Kernel = Poly :\u001b[0m 0.14285714285714285\n",
      "Accuracy Percentage: 14.285714285714285%\n",
      "Category: food\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "\n",
      "Category: Clothing\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "\n",
      "Category: Beauty\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "\n",
      "Category: GiftShop\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "\n",
      "Category: Fitness\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the Excel sheet into a DataFrame\n",
    "df = pd.read_excel('SVM_modeltesting_results.xlsx')\n",
    "bold_start = \"\\033[1m\"\n",
    "bold_end = \"\\033[0m\"\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_accuracy = (df['predicted_category'] == df['actualcategory']).mean()\n",
    "print(bold_start+\"**********************SVM Model Performance***************************\\n\"+bold_end)\n",
    "print(bold_start+ \"Overall Accuracy of SVM Model using Kernel = Poly :\"+ bold_end, overall_accuracy)\n",
    "print(\"Accuracy Percentage: {:}%\".format(overall_accuracy*100))\n",
    "\n",
    "\n",
    "# Calculate metrics for each category\n",
    "categories = df['actualcategory'].unique()\n",
    "for category in categories:\n",
    "    # Filter the DataFrame for the current category\n",
    "    category_df = df[df['actualcategory'] == category]\n",
    "    \n",
    "    #For Multi-Class Classification to get Precision,Recall,F1_score we have to set average='binary'\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = precision_score(category_df['actualcategory'], category_df['predicted_category'], average='macro')\n",
    "    recall = recall_score(category_df['actualcategory'], category_df['predicted_category'], average='macro')\n",
    "    f1 = f1_score(category_df['actualcategory'], category_df['predicted_category'], average='macro')\n",
    "    \n",
    "    # Print the metrics for the current category\n",
    "    print(\"Category:\", category)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d97624",
   "metadata": {},
   "source": [
    "# Training the Model on Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "aa346d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['random_forest_model.joblib']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "# Load the data from the CSV file\n",
    "data = pd.read_excel('trainingset.xlsx')\n",
    "\n",
    "# Split the data into input features and labels\n",
    "X = data.iloc[:, 1:-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Create a random forest classifier object\n",
    "rf = RandomForestClassifier(n_estimators=30, random_state=42)\n",
    "\n",
    "# Train the random forest model on the data\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(rf, 'random_forest_model.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f511fce",
   "metadata": {},
   "source": [
    "# Testing the Model on Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "068c740b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   recipe  foodie  chicken  food  homemade  place  biryani  platter  box  \\\n",
      "0       4       0       16     1         0      1        0        0    0   \n",
      "1       0       0        0     0         0      0        0        0    0   \n",
      "2       0       0        0     0         0      0        0        0    0   \n",
      "3       5       1        4    26         0      1        0        0    0   \n",
      "4       0       0        0     0         0      0        0        0    0   \n",
      "5       0       0        0     0         0      0        0        0   14   \n",
      "6       0       0        0     0         0      0        0        0    0   \n",
      "\n",
      "   loved  gift  pakistan  lahore  mahionlinegift  giftboutique  day  \\\n",
      "0      0     0         0       0               0             0    0   \n",
      "1      0     0         0       0               0             0    2   \n",
      "2      0     0        35       1               0             0    0   \n",
      "3      0     0         0       3               0             0    0   \n",
      "4      0     0        10       3               0             0    0   \n",
      "5      0    34         0       7               0             0    3   \n",
      "6      0     0         4       0               0             0    7   \n",
      "\n",
      "   embroidery  dress  summer  eid  radiates  perfect  ochre  online  makeup  \\\n",
      "0           0      0       0    0         0        0      0       0       0   \n",
      "1           0      3       4    0         0        0      0       0       0   \n",
      "2           0      6       0    0         0        0      0       0       0   \n",
      "3           0      0       0    0         0        2      0       0       0   \n",
      "4           0      0       0    0         0        1      0       3       8   \n",
      "5           0      0       0    1         0        0      0       0       0   \n",
      "6           0      0       0    0         0        0      0       0       0   \n",
      "\n",
      "   makeupartist  beauty  concealer  skincare  ofraglow  highlighter  \\\n",
      "0             0       0          0         0         0            0   \n",
      "1             0       0          0         0         0            0   \n",
      "2             0       0          0         0         0            0   \n",
      "3             0       0          0         0         0            0   \n",
      "4             1       5          0         0         0            1   \n",
      "5             0       0          0         0         0            0   \n",
      "6             0       0          0         0         0            0   \n",
      "\n",
      "   crueltyfreemakeup  fitness  exercise  fat  need  weightlossjourney  \\\n",
      "0                  0        0         0    0     0                  0   \n",
      "1                  0        0         0    0     0                  0   \n",
      "2                  0        0         0    0     0                  0   \n",
      "3                  0        0         0    0     0                  0   \n",
      "4                  0        0         0    0     0                  0   \n",
      "5                  0        0         0    0     0                  0   \n",
      "6                  0        8         2    4     1                  0   \n",
      "\n",
      "   bodypositivity  lowcarbvegan  difference  \n",
      "0               0             0           0  \n",
      "1               0             0           0  \n",
      "2               0             0           0  \n",
      "3               0             0           0  \n",
      "4               0             0           0  \n",
      "5               0             0           0  \n",
      "6               0             0           0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "# Load the model\n",
    "model = joblib.load('random_forest_model.joblib')\n",
    "\n",
    "# Load the testing data\n",
    "testing_df = pd.read_excel(\"modeltesting.xlsx\")\n",
    "\n",
    "# Use all columns except the first as input features\n",
    "features_df = testing_df.iloc[:, 1:]\n",
    "\n",
    "#print(features_df)\n",
    "# Predict the category for each row using the model\n",
    "predictions = model.predict(features_df)\n",
    "\n",
    "# Add the predicted category to the original testing dataframe\n",
    "testing_df[\"predicted_category\"] = predictions\n",
    "\n",
    "# Save the results to a new excel file\n",
    "testing_df.to_excel(\"randomforest_modeltesting_results.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3464456",
   "metadata": {},
   "source": [
    "#  Appending the Actual Category of the Pages of Testing File in the Random Forest Model Results File to get Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3f5add21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the original DataFrame from the file\n",
    "df = pd.read_excel('testingset.xlsx')\n",
    "\n",
    "# Get the last column (actual category) and the username column\n",
    "last_column = df.iloc[:, -1]\n",
    "username_column = df['username']\n",
    "\n",
    "# Create a dictionary to store unique username and corresponding actual category\n",
    "username_category_dict = {}\n",
    "\n",
    "# Iterate over the username and last column\n",
    "for username, category in zip(username_column, last_column):\n",
    "    if username not in username_category_dict:\n",
    "        # If the username is not already in the dictionary, add it with the corresponding category\n",
    "        username_category_dict[username] = category\n",
    "\n",
    "# Read the SVMresults.xlsx file\n",
    "result_df = pd.read_excel('randomforest_modeltesting_results.xlsx')\n",
    "\n",
    "# Create a new column to store the actual category based on unique username\n",
    "result_df['actualcategory'] = result_df['username'].map(username_category_dict)\n",
    "\n",
    "# Write the updated DataFrame back to the SVMresults.xlsx file\n",
    "result_df.to_excel('randomforest_modeltesting_results.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5693598d",
   "metadata": {},
   "source": [
    "# Calculating Performance Metric of Model Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9d1499c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m***********Random Forest Model Performance using 30 Trees and 8 Topics**************\u001b[0m\n",
      "\u001b[1mOverall Accuracy of the Model:\u001b[0m 1.0\n",
      "Accuracy Percentage: 100.0%\n",
      "\u001b[1mCategory\u001b[0m: \u001b[1mfood\u001b[0m\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "\n",
      "\u001b[1mCategory\u001b[0m: \u001b[1mClothing\u001b[0m\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "\n",
      "\u001b[1mCategory\u001b[0m: \u001b[1mBeauty\u001b[0m\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "\n",
      "\u001b[1mCategory\u001b[0m: \u001b[1mGiftShop\u001b[0m\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "\n",
      "\u001b[1mCategory\u001b[0m: \u001b[1mFitness\u001b[0m\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "bold_start = \"\\033[1m\"\n",
    "bold_end = \"\\033[0m\"\n",
    "\n",
    "\n",
    "# Load the Excel sheet into a DataFrame\n",
    "df = pd.read_excel('randomforest_modeltesting_results.xlsx')\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_accuracy = (df['predicted_category'] == df['actualcategory']).mean()\n",
    "#print(\"**************************************************************************\")\n",
    "print(bold_start+\"***********Random Forest Model Performance using 30 Trees and 8 Topics**************\"+bold_end)\n",
    "#print(\"***************************************************************************\\n\")\n",
    "print(bold_start+\"Overall Accuracy of the Model:\"+bold_end, overall_accuracy)\n",
    "print(\"Accuracy Percentage: {:}%\".format(overall_accuracy*100))\n",
    "\n",
    "# Calculate metrics for each category\n",
    "categories = df['actualcategory'].unique()\n",
    "for category in categories:\n",
    "    # Filter the DataFrame for the current category\n",
    "    category_df = df[df['actualcategory'] == category]\n",
    "    \n",
    "    #For Multi-Class Classification to get Precision,Recall,F1_score we have to set average='binary'\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    # Calculate precision for all classes\n",
    "    class_accuracy = accuracy_score(category_df['actualcategory'], category_df['predicted_category'])\n",
    "\n",
    "    precision = precision_score(y_true=df['actualcategory'], y_pred=df['predicted_category'], average='macro')\n",
    "\n",
    "    recall = recall_score(category_df['actualcategory'], category_df['predicted_category'], average='macro')\n",
    "    #print(\"Recall:\", recall)\n",
    "\n",
    "    f1 = f1_score(category_df['actualcategory'], category_df['predicted_category'], average='macro')\n",
    "    \n",
    "    # Print the metrics for the current category\n",
    "        # Define ANSI escape sequence for bold text\n",
    "\n",
    "    text = \"Category\"\n",
    "\n",
    "    print(bold_start + text + bold_end+\":\", bold_start + category + bold_end)\n",
    "    print(\"Accuracy:\", class_accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9c9bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
